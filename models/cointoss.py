'''
Apply basic Variational Inference to a coin toss example.
Inspired by `Variational Coin Toss<http://www.openias.org/variational-coin-toss>`_

Coin tosses :math:`x` are generated by a Bernoulli process with unknown heads probability :math:`z = P(x=1)`,
so z is the latent/hidden variable, and we want to estimate the posterior distribution of z
based on observations of the tosses x.
'''
import numpy as np

import scipy.stats as stats
from scipy.special import gammaln
import torch as t
import torch.nn as nn
from torch.autograd import Variable, backward
import torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns

def gen_tosses(z, n=1):
    return np.random.binomial(1, z, n)

def gen_z_beta(a=1,b=1, n=1):
    return np.random.beta(a,b,n)

def gen_z_beta_tosses(nb=1, bsiz=1, a=1, b=1):
    '''
    :param a:
    :param b:
    :param nb: num mini-batches
    :param bsiz: num tosses in a mini-batch
    :return: FloatTensor (nb*bsiz) x 1
    '''

    z = gen_z_beta(a,b,1)

    return Variable(t.FloatTensor(np.array(gen_tosses(z, nb*bsiz)*1.0)).unsqueeze(1))

def logit(x):
    return t.log(x/(1-x))

def sigmoid(x):
    return 1.0/(1 + np.exp(-x))

def rand_logit_normal(n=1, mu = 0.5, sig = 0.5):
    return sigmoid(np.random.normal(mu, sig, n))

# MLE estimate of heads-prob z given tosses x
def mle(x):
    return (x.sum()/(1+x*0).sum()).data.numpy()[0]

def beta_post(x):
    '''
    Beta-posterior of heads-prob z given prior p(z) = Beta(z; a,b), and observed tosses x
    '''


class VarToss(nn.Module):
    def __init__(self, ns=10, a = 3, b = 3, seed = 12345, variational=True, post = 'logit-normal'):
        '''
        Compute ELBO using Monte Carlo with 'ns' samples
        '''
        super(VarToss, self).__init__()
        self.post = post
        self.a = a
        self.b = b
        self.variational = variational
        self.samples = Variable(t.randn(ns,1)) # frozen standard normals !
        self.mu  =  Variable(t.FloatTensor([0.5]), requires_grad = True)
        self.sig =  Variable(t.FloatTensor([0.5]), requires_grad = True)

    def gen_tosses(self, n):
        z = t.FloatTensor(np.random.beta(self.a, self.b, size = n))
        return Variable(t.bernoulli(z).unsqueeze(1))

    def z_samples(self):
        '''
        Samples from posterior distribution parametrized by mu, sig
        '''
        if self.post == 'logit-normal':
            # logit-normal distribution with mu, sig
            mu = self.mu.expand_as(self.samples)
            sig = self.sig.expand_as(self.samples)
            return t.sigmoid(mu + sig * self.samples)
        elif self.post == 'const': # constant: mu is our only parameter!
            return self.mu.view(1,1)
        elif self.post == 'logistic':
            return t.sigmoid(self.mu).view(1,1)


    def log_beta_pdf(self, z, nh=0.0, nt = 0.0):
        '''
        log of beta-density; we need this explicit form (rather than using scipy/numpy) because
        it all needs to be part of PyTorch's tensor-based functions.
        '''
        return (self.a + nh - 1.0) * t.log(z) + (self.b + nt - 1.0) * t.log(1 - z) + \
               gammaln(self.a + self.b + nh + nt) - gammaln(self.a + nh) - gammaln(self.b + nt)

    def log_logit_norm_pdf(self, z):
        mu = self.mu.expand_as(z)
        sig = self.sig.expand_as(z)
        return -(logit(z) - mu)**2/(2 * sig*sig) - t.log((z*(1-z))) - t.log(sig) - np.log(np.sqrt(2 * np.pi))


    def kl(self):
        '''
        KL(q||p), which doesn't depend on inputs x;
        q = distr approximating the true posterior p(z|x)
        p = assumed prior p(z)
        Essentially this term is a "regularizer" that keeps the produced q
        from straying too far from the prior p
        :return:
        '''
        if self.post != 'logit-normal':
            raise ValueError('kl div regularizer only valid for logit-normal posterior!')
        z = self.z_samples()
        log_logit_norm_densities = self.log_logit_norm_pdf(z) # our posterior model for q(z) := p(z|x)
        log_beta_densities = self.log_beta_pdf(z)  # prior p(z)
        return (log_logit_norm_densities - log_beta_densities).mean()

    def log_likelihood(self, x):
        '''
        mean (over x_i and samples z_j ) of log( p(x_i | z_j) )
        '''
        nx = len(x)
        z = self.z_samples()
        nz = len(z)
        z_rep = z.transpose(1,0).repeat(nx,1)
        x_rep = x.repeat(1,nz)
        return (x_rep * t.log(z_rep) + (1 - x_rep) * t.log(1 - z_rep)).mean()

    def parameters(self):
        '''
        Normally don't need this but this is a non-traditional network
        so we override parameters() so it returns the two params to optimize
        '''
        return [self.mu, self.sig]

    def forward(self, x):
        loss = -self.log_likelihood(x)
        if self.variational:
            loss += self.kl()
        return loss


nb = 1
bsiz = 30
epochs =500

vt = VarToss(ns=10, a=4, b=8, post = 'logit-normal', variational=True)
#x = vt.gen_tosses(nb*bsiz)
x = gen_z_beta_tosses(nb, bsiz,a=4, b=8)
kl = vt.kl()

opt = optim.Adam(vt.parameters(), lr = 0.01)
#opt = optim.SGD(vt.parameters(), lr = 0.001)
#opt = optim.RMSprop(vt.parameters(), lr = 0.01)
# do one pass just to test
# b = 0
# x_batch = x[b * bsiz:(b + 1) * bsiz, :]
# elbo = vt(x_batch)
# elbo.backward()
# opt.step()
# [p for p in vt.parameters()]


zvals = Variable(t.FloatTensor(np.arange(0, 1, 0.01)).unsqueeze(1))
nh = int(x.sum().data.numpy()[0])
nt = int((1 - x).sum().data.numpy()[0])

p_z_x_pdf = t.exp(vt.log_beta_pdf(zvals, nh=nh, nt=nt)).data.numpy()


f = plt.figure()
p1 = f.add_subplot(111)
p2 = f.add_subplot(111)



for e in range(epochs):
    for b in range(nb):
        opt.zero_grad()
        vt.zero_grad()
        x_batch = x[b*bsiz:(b+1)*bsiz, :]
        elbo = vt(x_batch)
        elbo.backward()
        opt.step()

    if e % 5 == 0:
        q_z_pdf = t.exp(vt.log_logit_norm_pdf(zvals)).data.numpy()
        p2.clear()
        p1.plot(zvals.data.numpy(), p_z_x_pdf)
        p2.plot(zvals.data.numpy(), q_z_pdf)
        plt.pause(1)
        f.show()

        print('elbo= %8.3f mu=%5.2f sig=%5.2f  g.mu= %8.3f  g.sig= %8.3f mle= %5.2f z= %5.2f' %
          (elbo.data.numpy()[0],
           vt.mu.data.numpy()[0],
           vt.sig.data.numpy()[0],
           vt.mu.grad.data.numpy()[0],
           vt.sig.grad.data.numpy()[0] if vt.sig.grad else 0.0,
           mle(x),
           vt.z_samples().mean().data.numpy()[0]
           ))

vt.parameters()
# sns.distplot(rand_logit_normal(1000, vt.mu.data.numpy()[0], vt.sig.data.numpy()[0]))
# sns.distplot(gen_z_beta(a = (vt.a + x.sum()).data.numpy()[0],
#                         b = (vt.b + (1-x).sum()).data.numpy()[0],  n = 1000))



